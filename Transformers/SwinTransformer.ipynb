{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin Transformer Model \n",
    "Taken from [here](https://python.plainenglish.io/swin-transformer-from-scratch-in-pytorch-31275152bf03)\n",
    "\n",
    "Task: Image Classification\n",
    "\n",
    "Microsoft proposed the Swin-Transformer which features a local attention mechanism based on shifting windows whose computational complexity scales linearly and could serve as an all-purpose backbone for general vision tasks.\n",
    "\n",
    "The model starts by splitting an image into p x p non-overlapping patches with a linear embedding exactly like ViT. Our image transforms from (h,w,c) to (h/p,w/p,c*p**2) from patch partitioning, and then to (h/p * w/p, C) after the linear projection. We treat the h*w patches as the tokens of the transformer sequence and C as our embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Partition + Linear Embedding\n",
    "\n",
    "“It first splits an input RGB image into non-overlapping patches by a patch splitting module, like ViT. *Each patch is treated as a “token”* and its feature is set as a concatenation of the raw pixel RGB values. In our implementation, we use a patch size of 4 × 4 and thus the feature dimension of each patch is 4 × 4 × 3 = 48. A linear embedding layer is applied on this raw-valued feature to project it to an arbitrary dimension (denoted as C).”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinEmbedding(nn.Module):\n",
    "\n",
    "    '''\n",
    "    input shape -> (b,c,h,w)\n",
    "    output shape -> (b, (h/4 * w/4), C)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, patch_size=4, C=96):\n",
    "        super().__init__()\n",
    "        self.linear_embedding = nn.Conv2d(3, C, kernel_size=patch_size, stride=patch_size)\n",
    "        self.layer_norm = nn.LayerNorm(C)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.linear_embedding(x)\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.relu(self.layer_norm(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch Merging Layer\n",
    "\n",
    "\"To produce a hierarchical representation, the number of tokens is reduced by patch merging layers as the network gets deeper. The first patch merging layer concatenates the features of each group of 2 × 2 neighboring patches, and applies a linear layer on the 4C-dimensional concatenated features. This reduces the number of tokens by a multiple of 2×2 = 4 (2× downsampling of resolution), and the output dimension is set to 2C.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "\n",
    "    '''\n",
    "    input shape -> (b, (h*w), C)\n",
    "    output shape -> (b, (h/2 * w/2), C*2)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4*C, 2*C)\n",
    "        self.layer_norm = nn.LayerNorm(2*C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        height = width = int(math.sqrt(x.shape[1])/2)\n",
    "        x = rearrange(x, 'b (h s1 w s2) c -> b (h w) (s2 s1 c)', s1=2, s2=2, h=height, w=width)\n",
    "        return self.layer_norm(self.linear(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIndow Attenton Mechanism\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:640/format:webp/1*Kgi0npIhx7pdSBddP5m28A.png'>\n",
    "\n",
    "In the Swin Transformer, attention is computed with the familiar attention formula shown in the image above but in parallel across non-overlapping windows. We will start by first coding the standard window based self attention mechanism and we will deal with the alternating shifted windows later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedWindowMSA(nn.Module):\n",
    "\n",
    "    '''\n",
    "    input shape -> (b,(h*w), C)\n",
    "    output shape -> (b, (h*w), C)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, window_size=7):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.proj1 = nn.Linear(embed_dim, 3*embed_dim)\n",
    "        self.proj2 = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_dim = self.embed_dim / self.num_heads\n",
    "        height = width = int(math.sqrt(x.shape[1]))\n",
    "        x = self.proj1(x)\n",
    "\n",
    "        x = rearrange(x, 'b (h w) (c K) -> b h w c K', K=3, h=height, w=width)\n",
    "        x = rearrange(x, 'b (h m1) (w m2) (H E) K -> b H h w (m1 m2) E K', H=self.num_heads, m1=self.window_size, m2=self.window_size)\n",
    "        \n",
    "        '''\n",
    "          H = # of Attention Heads\n",
    "          h,w = # of windows vertically and horizontally\n",
    "          (m1 m2) = total size of each window\n",
    "          E = head dimension\n",
    "          K = 3 = a constant to break our matrix into 3 Q,K,V matricies \n",
    "        '''\n",
    "\n",
    "        Q, K, V = x.chunk(3, dim=6)\n",
    "        Q, K, V = Q.squeeze(-1), K.squeeze(-1), V.squeeze(-1)\n",
    "        att_scores = (Q @ K.transpose(4,5)) / math.sqrt(h_dim)\n",
    "        att = F.softmax(att_scores, dim=-1) @ V\n",
    "\n",
    "        x = rearrange(att, 'b H h w (m1 m2) E -> b (h m1) (w m2) (H E)', m1=self.window_size, m2=self.window_size)\n",
    "        x = rearrange(x, 'b h w c -> b (h w) c')\n",
    "\n",
    "        return self.proj2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedWindowMSA(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, window_size=7, mask=False):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.mask = mask\n",
    "        self.proj1 = nn.Linear(embed_dim, 3*embed_dim)\n",
    "        self.proj2 = nn.Linear(embed_dim, embed_dim)\n",
    "        # self.embeddings = RelativeEmbeddings()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_dim = self.embed_dim / self.num_heads\n",
    "        height = width = int(math.sqrt(x.shape[1]))\n",
    "        x = self.proj1(x)\n",
    "        x = rearrange(x, 'b (h w) (c K) -> b h w c K', K=3, h=height, w=width)\n",
    "\n",
    "        if self.mask:\n",
    "            x = torch.roll(x, (-self.window_size//2, -self.window_size//2), dims=(1,2))\n",
    "\n",
    "        x = rearrange(x, 'b (h m1) (w m2) (H E) K -> b H h w (m1 m2) E K', H=self.num_heads, m1=self.window_size, m2=self.window_size)\n",
    "        Q, K, V = x.chunk(3, dim=6)\n",
    "        Q, K, V = Q.squeeze(-1), K.squeeze(-1), V.squeeze(-1)\n",
    "        att_scores = (Q @ K.transpose(4,5)) / math.sqrt(h_dim)\n",
    "        # att_scores = self.embeddings(att_scores)\n",
    "\n",
    "        '''\n",
    "          shape of att_scores = (b, H, h, w, (m1*m2), (m1*m2))\n",
    "          we simply have to generate our row/column masks and apply them\n",
    "          to the last row and columns of windows which are [:,:,-1,:] and [:,:,:,-1]\n",
    "        '''\n",
    "\n",
    "        if self.mask:\n",
    "            row_mask = torch.zeros((self.window_size**2, self.window_size**2)).cuda()\n",
    "            row_mask[-self.window_size * (self.window_size//2):, 0:-self.window_size * (self.window_size//2)] = float('-inf')\n",
    "            row_mask[0:-self.window_size * (self.window_size//2), -self.window_size * (self.window_size//2):] = float('-inf')\n",
    "            column_mask = rearrange(row_mask, '(r w1) (c w2) -> (w1 r) (w2 c)', w1=self.window_size, w2=self.window_size).cuda()\n",
    "            att_scores[:, :, -1, :] += row_mask\n",
    "            att_scores[:, :, :, -1] += column_mask\n",
    "\n",
    "        att = F.softmax(att_scores, dim=-1) @ V\n",
    "        x = rearrange(att, 'b H h w (m1 m2) E -> b (h m1) (w m2) (H E)', m1=self.window_size, m2=self.window_size)\n",
    "\n",
    "        if self.mask:\n",
    "            x = torch.roll(x, (self.window_size//2, self.window_size//2), (1,2))\n",
    "\n",
    "        x = rearrange(x, 'b h w c -> b (h w) c')\n",
    "        return self.proj2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelativeEmbeddings(nn.Module):\n",
    "    def __init__(self, window_size=7):\n",
    "        super().__init__()\n",
    "        B = nn.Parameter(torch.randn(2*window_size-1, 2*window_size-1))\n",
    "        x = torch.arange(1,window_size+1,1/window_size)\n",
    "        x = (x[None, :]-x[:, None]).int()\n",
    "        y = torch.concat([torch.arange(1,window_size+1)] * window_size)\n",
    "        y = (y[None, :]-y[:, None])\n",
    "        self.embeddings = nn.Parameter((B[x[:,:], y[:,:]]), requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, window_size, mask):\n",
    "        super().__init__()\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.WMSA = ShiftedWindowMSA(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=mask)\n",
    "        self.MLP1 = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim*4, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        height, width = x.shape[1:3]\n",
    "        res1 = self.dropout(self.WMSA(self.layer_norm(x)) + x)\n",
    "        x = self.layer_norm(res1)\n",
    "        x = self.MLP1(x)\n",
    "        return self.dropout(x + res1)\n",
    "    \n",
    "class AlternatingEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, window_size=7):\n",
    "        super().__init__()\n",
    "        self.WSA = SwinEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=False)\n",
    "        self.SWSA = SwinEncoderBlock(embed_dim=embed_dim, num_heads=num_heads, window_size=window_size, mask=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.SWSA(self.WSA(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Embedding = SwinEmbedding()\n",
    "        self.Embedding = SwinEmbedding()\n",
    "        self.PatchMerge1 = PatchMerging(96)\n",
    "        self.PatchMerge2 = PatchMerging(192)\n",
    "        self.PatchMerge3 = PatchMerging(384)\n",
    "        self.Stage1 = AlternatingEncoderBlock(96, 3)\n",
    "        self.Stage2 = AlternatingEncoderBlock(192, 6)\n",
    "        self.Stage3_1 = AlternatingEncoderBlock(384, 12)\n",
    "        self.Stage3_2 = AlternatingEncoderBlock(384, 12)\n",
    "        self.Stage3_3 = AlternatingEncoderBlock(384, 12)\n",
    "        self.Stage4 = AlternatingEncoderBlock(768, 24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Embedding(x)\n",
    "        x = self.PatchMerge1(self.Stage1(x))\n",
    "        x = self.PatchMerge2(self.Stage2(x))\n",
    "        x = self.Stage3_1(x)\n",
    "        x = self.Stage3_2(x)\n",
    "        x = self.Stage3_3(x)\n",
    "        x = self.PatchMerge3(x)\n",
    "        x = self.Stage4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 49, 768])\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    x = torch.randn((1,3,224,224)).cuda()\n",
    "    model = SwinTransformer().cuda()\n",
    "    print(model(x).shape)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Because we will pass the image through the initial embedding layer and 3 patch merging layers, we expect our final shape (1,49,768) where 1 is the batch dimension, 49 is the 7*7 height and width reshaped into one dimension, and 768 is the final number of channels/ embedding dimension size.\n",
    "\n",
    "What does this dimension really represent ? I really need to understand better these things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
