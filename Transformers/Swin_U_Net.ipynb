{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swin_U_Net \n",
    "\n",
    "Take from [here](https://medium.com/@ashishbisht0307/swin-transformer-based-unet-architecture-for-semantic-segmentation-with-pytorch-code-91e779334e8e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(4 * dim)\n",
    "        self.reduction = nn.Linear(4*dim, 2*dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "        x = x.reshape(B, H // 2, 2, W // 2, 2, C).permute(0, 1, 3, 4, 2, 5).flatten(3)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExpansion(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim//2)\n",
    "        self.expand = nn.Linear(dim, 2*dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.expand(x)\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        x = x.view(B, H , W, 2, 2, C//4)\n",
    "        x = x.permute(0,1,3,2,4,5)\n",
    "\n",
    "        x = x.reshape(B,H*2, W*2 , C//4)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinBlock(nn.Module):\n",
    "    def __init__(self, dims, ip_res, ss_size = 3):\n",
    "        super().__init__()\n",
    "        self.swtb1 = SwinTransformerBlock(dim=dims, input_resolution=ip_res)\n",
    "        self.swtb2 = SwinTransformerBlock(dim=dims, input_resolution=ip_res, shift_size=ss_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.swtb2(self.swtb1(x))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, C, partioned_ip_res, num_blocks=3):\n",
    "        super().__init__()\n",
    "        H,W = partioned_ip_res[0], partioned_ip_res[1]\n",
    "        self.enc_swin_blocks = nn.ModuleList([\n",
    "            SwinBlock(C, (H, W)),\n",
    "            SwinBlock(2*C, (H//2, W//2)),\n",
    "            SwinBlock(4*C, (H//4, W//4))\n",
    "        ])\n",
    "        self.enc_patch_merge_blocks = nn.ModuleList([\n",
    "            PatchMerging(C),\n",
    "            PatchMerging(2*C),\n",
    "            PatchMerging(4*C)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_conn_ftrs = []\n",
    "        for swin_block,patch_merger in zip(self.enc_swin_blocks, self.enc_patch_merge_blocks):\n",
    "            x = swin_block(x)\n",
    "            skip_conn_ftrs.append(x)\n",
    "            x = patch_merger(x)\n",
    "        return x, skip_conn_ftrs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, C, partioned_ip_res, num_blocks=3):\n",
    "        super().__init__()\n",
    "        H,W = partioned_ip_res[0], partioned_ip_res[1]\n",
    "        self.dec_swin_blocks = nn.ModuleList([\n",
    "            SwinBlock(4*C, (H//4, W//4)),\n",
    "            SwinBlock(2*C, (H//2, W//2)),\n",
    "            SwinBlock(C, (H, W))\n",
    "        ])\n",
    "        self.dec_patch_expand_blocks = nn.ModuleList([\n",
    "            PatchExpansion(8*C),\n",
    "            PatchExpansion(4*C),\n",
    "            PatchExpansion(2*C)\n",
    "        ])\n",
    "        self.skip_conn_concat = nn.ModuleList([\n",
    "            nn.Linear(8*C, 4*C),\n",
    "            nn.Linear(4*C, 2*C),\n",
    "            nn.Linear(2*C, 1*C)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, encoder_features):\n",
    "        for patch_expand,swin_block, enc_ftr, linear_concatter in zip(self.dec_patch_expand_blocks, self.dec_swin_blocks, encoder_features,self.skip_conn_concat):\n",
    "            x = patch_expand(x)\n",
    "            x = torch.cat([x, enc_ftr], dim=-1)\n",
    "            x = linear_concatter(x)\n",
    "            x = swin_block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinUNet(nn.Module):\n",
    "    def __init__(self, H, W, ch, C, num_class, num_blocks=3, patch_size = 4):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(ch, C, patch_size)\n",
    "        self.encoder = Encoder(C, (H//patch_size, W//patch_size),num_blocks)\n",
    "        self.bottleneck = SwinBlock(C*(2**num_blocks), (H//(patch_size* (2**num_blocks)), W//(patch_size* (2**num_blocks))))\n",
    "        self.decoder = Decoder(C, (H//patch_size, W//patch_size),num_blocks)\n",
    "        self.final_expansion = FinalPatchExpansion(C)\n",
    "        self.head        = nn.Conv2d(C, num_class, 1,padding='same')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        x,skip_ftrs  = self.encoder(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        x = self.decoder(x, skip_ftrs[::-1])\n",
    "\n",
    "        x = self.final_expansion(x)\n",
    "\n",
    "        x = self.head(x.permute(0,3,1,2))\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
