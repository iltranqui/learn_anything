{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are We Building?\n",
    "We need to create an efficient data loading method to train the image classification algorithm that automates preprocessing the images and assigning labels to the training samples. In this article, we perform image classification on the CIFAR-10 dataset.\n",
    "\n",
    "A typical split for the CIFAR-10 dataset uses 50,000 images for training and 10,000 for testing.\n",
    "\n",
    "We are using the CIFAR-10 dataset, which contains a total of 60000 32x32 RGB images with 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cifar10(Dataset):\n",
    "    def __init__(self, *args):\n",
    "    \n",
    "    # initialize dataset variables here\n",
    "        \n",
    "    def __len__(self):\n",
    "        # returns the length of the dataset \n",
    "        return None    \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # preprocess and transformations\n",
    "        # indexes the dataset such that dataset[i] can retrieve the ith sample.\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cifar10(Dataset):\n",
    "\tdef __init__(self, root, train = False, transforms = None):\n",
    "        # Variables for the dataset\n",
    "\t\tself.root = root\n",
    "\t\tself.transforms = transforms\n",
    "\t\tself.train = train\n",
    "\n",
    "        # Splitting the dataset into training and test data \n",
    "\t\tself.train_data = [file for file in os.listdir(root) if \"data_batch\" in file]\n",
    "\t\tself.test_data = [file for file in os.listdir(root) if \"test_batch\" in file]\n",
    "\n",
    "\t\tself.data_files = self.train_data if self.train else self.test_data\n",
    "\t\t# we will read the images and labels and store them in these lists.\n",
    "\t\tself.images = [] # -> list\n",
    "\t\tself.labels = [] # -> list\n",
    "\t\t\n",
    "\t\tself.load_data()\n",
    "\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\t# this returns and only returns the length of the dataset\n",
    "\t\treturn len(self.images)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\n",
    "\t\tif torch.is_tensor(idx):\n",
    "\t\t\tidx = idx.tolist()\n",
    "\n",
    "        # retrieve the image from the list created of the dataset\n",
    "\t\timage = self.images[idx]\n",
    "\t\timage = Image.fromarray(image) # -> return it as an image\n",
    "\n",
    "\t\tlabel = self.labels[idx]  # -> get the labels of the image\n",
    "\n",
    "\t\tif self.transforms:\n",
    "\t\t\timage = self.transforms(image)   # apply any transformation\n",
    "\n",
    "\t\treturn image, label  # -> return the image and the labels as a tuple\n",
    "\n",
    "\n",
    "\tdef load_data(self):\n",
    "\n",
    "\t\tfor file in self.data_files:\n",
    "\t\t\tfile_path = os.path.join(self.root, file)\n",
    "\t\t\tsample = self.read_file(file_path)\n",
    "\t\t\tself.images.append(sample[\"data\"])\n",
    "\t\t\tself.labels.extend(sample[\"labels\"])\n",
    "\n",
    "\n",
    "\t\tself.images = np.vstack(self.images).reshape(-1, 3, 32, 32)\n",
    "\t\tself.images = self.images.transpose((0, 2, 3, 1))\n",
    "\n",
    "\tdef read_file(self, filename):\n",
    "\t\twith open(filename, \"rb\") as f:\n",
    "\t\t\tf = pickle.load(f, encoding = \"latin1\")\n",
    "\t\treturn f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations\n",
    "\n",
    "Image transformation is a technique in image recognition to pre-process and prepare images for machine learning algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transform.Compose([\n",
    "    transforms.Resize(200),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2023, 0.1994, 0.2010]),\n",
    "    transforms.RandomErasing(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomAffine(degrees=45, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=45),\n",
    "    transforms.RandomPerspective(),\n",
    "])\n",
    "\n",
    "# apply the transforms to an image\n",
    "image = Image.open('image.png')\n",
    "transformed_image = transform(image)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/main\" to /home/helldiver/.cache/torch/hub/main.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-18 from `Deep Residual Learning for Image Recognition <https://arxiv.org/abs/1512.03385>`__.\n",
      "\n",
      "    Args:\n",
      "        weights (:class:`~torchvision.models.ResNet18_Weights`, optional): The\n",
      "            pretrained weights to use. See\n",
      "            :class:`~torchvision.models.ResNet18_Weights` below for\n",
      "            more details, and possible values. By default, no pre-trained\n",
      "            weights are used.\n",
      "        progress (bool, optional): If True, displays a progress bar of the\n",
      "            download to stderr. Default is True.\n",
      "        **kwargs: parameters passed to the ``torchvision.models.resnet.ResNet``\n",
      "            base class. Please refer to the `source code\n",
      "            <https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py>`_\n",
      "            for more details about this class.\n",
      "\n",
      "    .. autoclass:: torchvision.models.ResNet18_Weights\n",
      "        :members:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
