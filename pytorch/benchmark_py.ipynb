{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference in Deep Learning Models ( PyTorch)\n",
    "\n",
    "Code and explanations taken from [here](https://deci.ai/blog/measure-inference-time-deep-neural-networks/)\n",
    "\n",
    "More specifically, when calling a function using a GPU, the operations are enqueued to the specific device, but not necessarily to other devices. This allows us to execute computations in parallel on the CPU or another GPU.\n",
    "\n",
    "<img src='https://deci.ai//wp-content/uploads/2020/05/Figure-1_white.png'>\n",
    "\n",
    "Asynchronous execution offers huge advantages for deep learning, such as the ability to decrease run-time by a large factor. For example, at the inference of multiple batches, the second batch can be preprocessed on the CPU while the first batch is fed forward through the network on the GPU. Clearly, it would be beneficial to use asynchronism whenever possible at inference time.\n",
    "\n",
    "When you calculate time with the “time” library in Python, the measurements are performed on the CPU device. Due to the asynchronous nature of the GPU, the line of code that stops the timing will be executed before the GPU process finishes. \n",
    "\n",
    "### GPU warm-up\n",
    "\n",
    "**GPU tend to stay in a passive state and not turned on ! So there is a wake-up time to boot the GPU when necessary called WAKEUP TIME**\n",
    "\n",
    "A modern GPU device can exist in one of several different power states. When the GPU is not being used for any purpose and persistence mode (i.e., which keeps the GPU on) is not enabled, the GPU will automatically reduce its power state to a very low level, sometimes even a complete shutdown. In lower power state, the GPU shuts down different pieces of hardware, including memory subsystems, internal subsystems, or even compute cores and caches.\n",
    "\n",
    "**the invocation of any program that attempts to interact with the GPU will cause the driver to load and/or initialize the GPU**\n",
    "\n",
    "#### In the code below\n",
    " Before we make any time measurements, we run some dummy examples through the network to do a ‘GPU warm-up.’ This will automatically initialize the GPU and prevent it from going into power-saving mode when we measure time.\n",
    "\n",
    " Next, we use tr.cuda.event to measure time on the GPU. It is crucial here to use torch.cuda.synchronize(). This line of code performs synchronization between the host and device (i.e., GPU and CPU), so the time recording takes place only after the process running on the GPU is finished. This overcomes the issue of unsynchronized execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "segformer_path = \n",
    "model = torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/helldiver/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Inference Time: 19.99448523203532 ms for 1 single batch\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EfficientNet = torch.hub.load('NVIDIA/DeepLearningExamples:torchhub', 'nvidia_efficientnet_b0', pretrained=True)\n",
    "\n",
    "model = EfficientNet\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "dummy_input = torch.randn(1, 3,224,224, dtype=torch.float).to(device)\n",
    "\n",
    "# INIT LOGGERS\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "repetitions = 300\n",
    "timings=np.zeros((repetitions,1))\n",
    "#GPU-WARM-UP\n",
    "for _ in range(10):\n",
    "    _ = model(dummy_input)\n",
    "# MEASURE PERFORMANCE\n",
    "with torch.no_grad():\n",
    "    for rep in range(repetitions):\n",
    "        starter.record()\n",
    "        _ = model(dummy_input)\n",
    "        ender.record()\n",
    "        # WAIT FOR GPU SYNC\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)\n",
    "        timings[rep] = curr_time\n",
    "\n",
    "mean_syn = np.sum(timings) / repetitions\n",
    "std_syn = np.std(timings)\n",
    "print(\"Mean Inference Time: {} ms for 1 single batch\".format(mean_syn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Throughput\n",
    "\n",
    "**How many inputs (images for Computer Vision) can the model output in a second based on the batch_size**\n",
    "\n",
    "The throughput of a neural network is defined as the maximal number of input instances the network can process in a unit of time (e.g., a second). Maybe the process is within a video or some other aspect. This allows you to determine the optimal batch size and the test to perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Throughput: 131.8970836079158 ---> The number of examples our network can process in one second\n",
      "\n",
      "Given that the network works with 16 batches in parallel\n",
      "\n",
      "Total time: 12.130670036315921\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "optimal_batch_size = 16\n",
    "dummy_input = torch.randn(optimal_batch_size, 3,224,224, dtype=torch.float).to(device)\n",
    "\n",
    "total_batches=100\n",
    "total_time = 0\n",
    "with torch.no_grad():\n",
    "    for rep in range(total_batches):\n",
    "        starter, ender = torch.cuda.Event(enable_timing=True),   torch.cuda.Event(enable_timing=True)\n",
    "        starter.record()\n",
    "        _ = model(dummy_input)\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        curr_time = starter.elapsed_time(ender)/1000\n",
    "        total_time += curr_time\n",
    "Throughput =   (total_batches*optimal_batch_size)/total_time\n",
    "print('Final Throughput: {} ---> The number of examples our network can process in one second'.format(Throughput))\n",
    "print(\"\\nGiven that the network works with {} batches in parallel\".format(optimal_batch_size))\n",
    "print(\"\\nTotal time: {}\".format(total_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
